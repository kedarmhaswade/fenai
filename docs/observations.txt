Image size:
-----------
Each image resized to nxn and then split into 64 squares. Each pixel has 3 values for RGB.
First started with n = 512, but that gives 12288 features for each square. This will need a lot of training data.
Tried with n = 256, 128 and 64. For 128 and 64, image accuracy is very low. So going head with n = 256.
With n = 256 number of features is still 768.

Data skew:
----------
With 11 chessboards, here is the distribution of classes:
    540 E
     37 P
     36 p
     13 R
     11 K
     11 k
      9 N
      9 n
      9 B
      8 r
      8 Q
      8 b
      5 q
Data is heavily skewed towards empty squares, followed by pawns.
To reduce the skew, selecting empty squares and pawns with a certain probability:
CLASS_SELECTION_PROB = {'p': 0.3, 'P': 0.3, 'E': 0.025}
After this, the distribution looks like the following:
     17 E
     13 R
     13 p
     11 K
     11 k
      9 P
      9 N
      9 n
      9 B
      8 r
      8 Q
      8 b
      5 q

Train-test split:
-----------------
70% train, 30% test

SVM classifier:
---------------
The default kernel rbf does not perform well. Even after reducing skew, it just predicts only one class for all - 'p'.
Rbf kernel accuracy is 0.05
Linear kernel accuracy is 0.54
Poly kernel accuracy is 0.44

Jun 30, 2023 - currently there are only 11 chessboards. As of now we need more data.