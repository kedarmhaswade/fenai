Image size:
-----------
Each image resized to nxn and then split into 64 squares. Each pixel has 3 values for RGB.
First started with n = 512, but that gives 12288 features for each square. This will need a lot of training data.
Tried with n = 256, 128 and 64. For 128 and 64, image accuracy is very low. So going head with n = 256.
With n = 256 number of features is still 768.

Data skew:
----------
With 11 chessboards, here is the distribution of classes:
    540 E
     37 P
     36 p
     13 R
     11 K
     11 k
      9 N
      9 n
      9 B
      8 r
      8 Q
      8 b
      5 q
Data is heavily skewed towards empty squares, followed by pawns.
To reduce the skew, selecting empty squares and pawns with a certain probability:
CLASS_SELECTION_PROB = {'p': 0.3, 'P': 0.3, 'E': 0.025}
After this, the distribution looks like the following:
     17 E
     13 R
     13 p
     11 K
     11 k
      9 P
      9 N
      9 n
      9 B
      8 r
      8 Q
      8 b
      5 q

Train-test split:
-----------------
70% train, 30% test

SVM classifier:
---------------
The default kernel rbf does not perform well. Even after reducing skew, it just predicts only one class for all - 'p'.
Rbf kernel accuracy is 0.05
Linear kernel accuracy is 0.54
Poly kernel accuracy is 0.44

Jun 30, 2023 - currently there are only 11 chessboards. As of now we need more data.

Jul 3, 2023 - added more handcrafted chessboards. Now there are 136 chessboards and 8767 squares.

With added data, and linear kernel the accuracy improved to about 60%.
Noticed that there were some bugs in FEN in name-fen-map.csv. Fixed the bugs and accuracy improved to 80%.

I was looking at the confusion matrix to see where the model is going wrong. I noticed that the train-test split does
not look good and the test set does not contain examples from some classes. I turned the shuffle parameter of
train_test_split on and now the accuracy is 99%.

With poly kernel, accuracy is 0.98.
Training takes a long time with rbf kernel. Rbf kernel performs really badly with only 19% accuracy and predicts
everything to be empty.

Tried linear kernel with test size 40%, 30%, 20% and 10%. Accuracy is 100% for 90-10 split and 0.99 for the rest.